{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Option 1\n",
    "\n",
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "import warnings\n",
    "import torch\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from itertools import product\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = \"Big Data Analytics\"\n",
    "master = \"local[*]\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    " .set('spark.driver.host','127.0.0.1')\\\n",
    "   .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties={}\n",
    "db_properties['username']=\"postgres\"\n",
    "db_properties['password']=\"\"\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "db_properties['table']=\"fifa.fifa\"\n",
    "db_properties['driver']=\"org.postgresql.Driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male = spark.read.csv('./Data/players_15.csv', header = True)\n",
    "combined_df = df_male.withColumn(\"year\", lit(2015))\n",
    "# combined_df = combined_df.withColumn(\"record_id\", monotonically_increasing_id()).select(\"record_id\", *combined_df.columns)\n",
    "combined_df = combined_df.withColumn(\"gender\", lit(\"Male\"))\n",
    "folder_path = \"./Data\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name == \"players_15.csv\":\n",
    "        continue\n",
    "    year = \"20\" + file_name[-6:-4]\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df_read = spark.read.csv(file_path, header = True)\n",
    "    df_read = df_read.withColumn(\"year\", lit(int(year)))\n",
    "    # df_read = df_read.withColumn(\"record_id\", monotonically_increasing_id()).select(\"record_id\", *df_read.columns)\n",
    "    if \"female\" in file_name:\n",
    "        df_read = df_read.withColumn(\"gender\", lit(\"Female\"))\n",
    "    else:\n",
    "        df_read = df_read.withColumn(\"gender\", lit(\"Male\"))\n",
    "    combined_df = combined_df.union(df_read)\n",
    "combined_df = combined_df.withColumn(\"record_id\", monotonically_increasing_id()).select(\"record_id\", *combined_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write to PostgreSQL\n",
    "combined_df.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from PostgreSQL to verify\n",
    "df_from_postgres = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_spark(spark, db_properties):\n",
    "    df_from_postgres = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", db_properties['url'])\\\n",
    "        .option(\"dbtable\", db_properties['table'])\\\n",
    "        .option(\"user\", db_properties['username'])\\\n",
    "        .option(\"password\", db_properties['password'])\\\n",
    "        .option(\"Driver\", db_properties['driver'])\\\n",
    "        .load()\n",
    "    df = df_from_postgres.filter(df_from_postgres[\"gender\"] == \"Male\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_clubs_with_contracts_ending(spark, db_properties, X, Y, Z):\n",
    "    df = read_from_spark(spark, db_properties)\n",
    "    df_filtered = df.filter(col(\"year\") == X)\n",
    "    df_expiring = df_filtered.filter(col(\"club_contract_valid_until\").cast(\"int\") >= Z)\n",
    "    result = df_expiring.groupBy(\"club_name\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .limit(Y)\n",
    "    return result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clubs_by_average_age(spark, db_properties, X, Y, highest=True):\n",
    "    if X <= 0:\n",
    "        return \"X must be a positive integer\"\n",
    "    if Y < 2015 or Y > 2022:\n",
    "        return \"Y must be a year between 2015 and 2022 inclusively\"\n",
    "    df = read_from_spark(spark, db_properties)\n",
    "    \n",
    "    # Filter data for specified year Y\n",
    "    df_filtered = df.filter(col(\"year\") == Y)\n",
    "    avg_age_per_club = df_filtered.groupBy(\"club_name\") \\\n",
    "        .agg(round(avg(\"age\").cast(\"float\"),2).alias(\"average_age\"))\n",
    "    if highest:\n",
    "        sorted_clubs = avg_age_per_club.orderBy(desc(\"average_age\"))\n",
    "    else:\n",
    "        sorted_clubs = avg_age_per_club.orderBy(asc(\"average_age\"))\n",
    "\n",
    "    top_clubs = sorted_clubs.limit(X)\n",
    "    last_club = top_clubs.collect()[-1]\n",
    "    threshold_age = last_club[\"average_age\"]\n",
    "    if highest:\n",
    "        result_clubs = sorted_clubs.filter(col(\"average_age\") >= threshold_age).collect()\n",
    "    else:\n",
    "        result_clubs = sorted_clubs.filter(col(\"average_age\") <= threshold_age).collect()\n",
    "    return result_clubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_popular_nationality(spark, db_properties):\n",
    "    df = read_from_spark(spark, db_properties)\n",
    "    # df_filtered = df.filter((col(\"year\") >= 2015) & (col(\"year\") <= 2022))\n",
    "    nationality_counts = df.groupBy(\"year\", \"nationality_name\") \\\n",
    "        .agg(count(\"*\").alias(\"count\"))\n",
    "    # Create a window partitioned by year and ordered by count descending\n",
    "    window = Window.partitionBy(\"year\").orderBy(desc(\"count\"))\n",
    "    \n",
    "    # Add row number within each year partition\n",
    "    ranked_nationalities = nationality_counts.withColumn(\"rank\", row_number().over(window))\n",
    "    # Filter for the top nationality for each year\n",
    "    most_popular_nationalities = ranked_nationalities.filter(col(\"rank\") == 1) \\\n",
    "        .select(\"year\", \"nationality_name\", \"count\") \\\n",
    "        .orderBy(\"year\")\n",
    "    \n",
    "    return most_popular_nationalities.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clubs = get_top_clubs_with_contracts_ending(spark=spark, db_properties=db_properties, X=2021, Y=10, Z=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(club_name='GwangJu FC', count=28),\n",
       " Row(club_name='Zamora Fútbol Club', count=27),\n",
       " Row(club_name='Club Plaza de Deportes Colonia', count=27),\n",
       " Row(club_name='SL Benfica', count=26),\n",
       " Row(club_name='Club Deportivo El Nacional', count=26),\n",
       " Row(club_name='Sociedad Deportiva Aucas', count=26),\n",
       " Row(club_name='Gangwon FC', count=26),\n",
       " Row(club_name='Club Atlético Nacional Potosí', count=26),\n",
       " Row(club_name='Busan IPark', count=26),\n",
       " Row(club_name='Club Sportivo Luqueño', count=25)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_clubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task-2.2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clubs_by_age = find_clubs_by_average_age(spark=spark, db_properties=db_properties, X=10, Y=2017, highest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(club_name='Sevilla Atlético', average_age=19.920000076293945),\n",
       " Row(club_name='Swindon Town', average_age=21.3700008392334),\n",
       " Row(club_name='CD Huachipato', average_age=21.40999984741211),\n",
       " Row(club_name='FC Nordsjælland', average_age=21.40999984741211),\n",
       " Row(club_name='FC Twente', average_age=21.59000015258789),\n",
       " Row(club_name='Envigado FC', average_age=21.610000610351562),\n",
       " Row(club_name='KRC Genk', average_age=21.6299991607666),\n",
       " Row(club_name='Crewe Alexandra', average_age=21.81999969482422),\n",
       " Row(club_name='Barnsley', average_age=21.8700008392334),\n",
       " Row(club_name='Ajax', average_age=21.969999313354492)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clubs_by_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task-2.3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_nationalities = get_most_popular_nationality(spark=spark, db_properties=db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=2015, nationality_name='England', count=1627),\n",
       " Row(year=2016, nationality_name='England', count=1519),\n",
       " Row(year=2017, nationality_name='England', count=1627),\n",
       " Row(year=2018, nationality_name='England', count=1633),\n",
       " Row(year=2019, nationality_name='England', count=1625),\n",
       " Row(year=2020, nationality_name='England', count=1670),\n",
       " Row(year=2021, nationality_name='England', count=1685),\n",
       " Row(year=2022, nationality_name='England', count=1719)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_nationalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Refer to preprocessing.ipynb for detailed investigation on correlations and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup column categories\n",
    "col_names = ['record_id',\n",
    " 'sofifa_id',\n",
    " 'player_url',\n",
    " 'short_name',\n",
    " 'long_name',\n",
    " 'player_positions',\n",
    " 'overall',\n",
    " 'potential',\n",
    " 'value_eur',\n",
    " 'wage_eur',\n",
    " 'age',\n",
    " 'dob',\n",
    " 'height_cm',\n",
    " 'weight_kg',\n",
    " 'club_team_id',\n",
    " 'club_name',\n",
    " 'league_name',\n",
    " 'league_level',\n",
    " 'club_position',\n",
    " 'club_jersey_number',\n",
    " 'club_loaned_from',\n",
    " 'club_joined',\n",
    " 'club_contract_valid_until',\n",
    " 'nationality_id',\n",
    " 'nationality_name',\n",
    " 'nation_team_id',\n",
    " 'nation_position',\n",
    " 'nation_jersey_number',\n",
    " 'preferred_foot',\n",
    " 'weak_foot',\n",
    " 'skill_moves',\n",
    " 'international_reputation',\n",
    " 'work_rate',\n",
    " 'body_type',\n",
    " 'real_face',\n",
    " 'release_clause_eur',\n",
    " 'player_tags',\n",
    " 'player_traits',\n",
    " 'pace',\n",
    " 'shooting',\n",
    " 'passing',\n",
    " 'dribbling',\n",
    " 'defending',\n",
    " 'physic',\n",
    " 'attacking_crossing',\n",
    " 'attacking_finishing',\n",
    " 'attacking_heading_accuracy',\n",
    " 'attacking_short_passing',\n",
    " 'attacking_volleys',\n",
    " 'skill_dribbling',\n",
    " 'skill_curve',\n",
    " 'skill_fk_accuracy',\n",
    " 'skill_long_passing',\n",
    " 'skill_ball_control',\n",
    " 'movement_acceleration',\n",
    " 'movement_sprint_speed',\n",
    " 'movement_agility',\n",
    " 'movement_reactions',\n",
    " 'movement_balance',\n",
    " 'power_shot_power',\n",
    " 'power_jumping',\n",
    " 'power_stamina',\n",
    " 'power_strength',\n",
    " 'power_long_shots',\n",
    " 'mentality_aggression',\n",
    " 'mentality_interceptions',\n",
    " 'mentality_positioning',\n",
    " 'mentality_vision',\n",
    " 'mentality_penalties',\n",
    " 'mentality_composure',\n",
    " 'defending_marking_awareness',\n",
    " 'defending_standing_tackle',\n",
    " 'defending_sliding_tackle',\n",
    " 'goalkeeping_diving',\n",
    " 'goalkeeping_handling',\n",
    " 'goalkeeping_kicking',\n",
    " 'goalkeeping_positioning',\n",
    " 'goalkeeping_reflexes',\n",
    " 'goalkeeping_speed',\n",
    " 'ls',\n",
    " 'st',\n",
    " 'rs',\n",
    " 'lw',\n",
    " 'lf',\n",
    " 'cf',\n",
    " 'rf',\n",
    " 'rw',\n",
    " 'lam',\n",
    " 'cam',\n",
    " 'ram',\n",
    " 'lm',\n",
    " 'lcm',\n",
    " 'cm',\n",
    " 'rcm',\n",
    " 'rm',\n",
    " 'lwb',\n",
    " 'ldm',\n",
    " 'cdm',\n",
    " 'rdm',\n",
    " 'rwb',\n",
    " 'lb',\n",
    " 'lcb',\n",
    " 'cb',\n",
    " 'rcb',\n",
    " 'rb',\n",
    " 'gk',\n",
    " 'player_face_url',\n",
    " 'club_logo_url',\n",
    " 'club_flag_url',\n",
    " 'nation_logo_url',\n",
    " 'nation_flag_url',\n",
    " 'year',\n",
    " 'gender']\n",
    "\n",
    "ordinal_cols = ['work_rate']\n",
    "nominal_cols = ['body_type']\n",
    "continuous_cols = ['potential', 'value_eur', 'wage_eur', 'age', 'height_cm', 'weight_kg',\n",
    "                   'weak_foot', 'skill_moves', 'international_reputation',\n",
    "                   'attacking_crossing','attacking_finishing', 'attacking_heading_accuracy', 'attacking_short_passing', 'attacking_volleys',\n",
    "                   'movement_sprint_speed', 'movement_agility', 'movement_reactions', 'movement_balance',\n",
    "                   'power_shot_power', 'power_jumping', 'power_stamina', 'power_strength', 'power_long_shots',\n",
    "                   'mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', 'mentality_penalties',\n",
    "                   'defending_marking_awareness', 'defending_standing_tackle', 'defending_sliding_tackle']\n",
    "\n",
    "main_traits_cols = ['pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic', \n",
    "                            'goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking',\n",
    "                            'goalkeeping_positioning', 'goalkeeping_reflexes', 'goalkeeping_speed']\n",
    "\n",
    "skill_cols = ['skill_dribbling', 'skill_curve', 'skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control']\n",
    "\n",
    "position_cols = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm',\n",
    "                   'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk']\n",
    "\n",
    "needed_cols = ['record_id', 'player_positions'] #needed for preprocessing but will be dropped\n",
    "\n",
    "columns_to_drop = ['sofifa_id', 'player_url', 'short_name', 'long_name',\n",
    "                   'dob', 'club_name', 'league_name', 'club_position', 'club_jersey_number', 'club_loaned_from',\n",
    "                   'club_joined','club_contract_valid_until', 'nationality_id', 'nationality_name', 'nation_team_id',\n",
    "                   'nation_position', 'nation_jersey_number', 'preferred_foot', 'real_face', 'player_tags', 'player_traits',\n",
    "                   'player_face_url', 'club_logo_url', 'club_flag_url', 'nation_logo_url', 'nation_flag_url',\n",
    "                   'year', 'gender', 'league_level']\n",
    "\n",
    "null_cols_drop = ['release_clause_eur']\n",
    "\n",
    "corr_cols_drop = ['movement_acceleration']\n",
    "\n",
    "dropped_null_rows = ['value_eur', 'wage_eur', 'trait6']\n",
    "\n",
    "imputed_cols = ['mentality_composure', 'club_team_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NullImputer(Transformer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset.withColumn(\"club_team_id_imputed\", when(col(\"club_team_id\").isNull(), -1).otherwise(col(\"club_team_id\")))\n",
    "        return output_df\n",
    "    \n",
    "class MLImputer(Transformer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        s = dataset.select(['record_id','mentality_composure', 'mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', 'mentality_penalties']).toPandas()\n",
    "       \n",
    "        # Prepare the data\n",
    "        train_data = s.dropna(subset=['mentality_composure'])\n",
    "        X = train_data[['mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', 'mentality_penalties']]\n",
    "        Y = train_data['mentality_composure']\n",
    "\n",
    "        # Fit the linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, Y)\n",
    "\n",
    "        # Predict missing values\n",
    "        X_missing = s[s['mentality_composure'].isnull()][['mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', 'mentality_penalties']]\n",
    "        predicted_values = model.predict(X_missing)\n",
    "        rounded_predicted_values = np.round(predicted_values).astype(int)\n",
    "        # Impute the predicted values\n",
    "        s.loc[s['mentality_composure'].isnull(), 'mentality_composure'] = rounded_predicted_values\n",
    "        s = s.rename(columns = {'mentality_composure': 'mentality_composure_imputed'})\n",
    "        spark_s = spark.createDataFrame(s[['record_id','mentality_composure_imputed']])\n",
    "        spark_df = dataset.join(spark_s, on = 'record_id', how = 'left')\n",
    "        return spark_df\n",
    "    \n",
    "class AverageSkillCreator(Transformer):\n",
    "    def __init__(self, average_skills_cols = None):\n",
    "        super().__init__()\n",
    "        self.average_skills_cols = average_skills_cols\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        spark_df = dataset\n",
    "        spark_df = spark_df.withColumn(\n",
    "        'corr_av_skills', \n",
    "        aggregate(\n",
    "            array(*[col(pos) for pos in self.average_skills_cols]),\n",
    "            lit(0.0),\n",
    "            lambda acc, x: acc + x,\n",
    "            lambda acc: acc / size(array(*[col(pos) for pos in self.average_skills_cols]))\n",
    "        )\n",
    "        )\n",
    "\n",
    "        return spark_df\n",
    "        \n",
    "class AveragePositionCreator(Transformer):\n",
    "    def __init__(self, position_cols = None):\n",
    "        super().__init__()\n",
    "        self.position_cols = position_cols\n",
    "\n",
    "    # Define UDF for safe evaluation\n",
    "    def safe_eval(self, x):\n",
    "        try:\n",
    "            return float(eval(str(x)))\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def _transform(self, dataset):\n",
    "        attacking_positions = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw',\n",
    "                           'lam', 'cam', 'ram']\n",
    "        midfield_positions = ['lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm',\n",
    "                            'ldm', 'cdm', 'rdm']\n",
    "        defensive_positions = ['rwb', 'lb', 'lcb', 'cb', 'lwb', 'rcb', 'rb',\n",
    "                            'ldm', 'cdm', 'rdm']\n",
    "        gk_positions = ['gk']\n",
    "\n",
    "        spark_df = dataset\n",
    "        safe_eval_udf = udf(self.safe_eval, FloatType())\n",
    "\n",
    "        #Apply safe_eval to each position\n",
    "        for pos in position_cols:\n",
    "            spark_df = spark_df.withColumn(pos, safe_eval_udf(pos))\n",
    "\n",
    "         #Add new columns 'average_val_attacking', 'average_val_midfield', 'average_val_defensive', 'average_val_gk'\n",
    "        spark_df = spark_df.withColumn(\n",
    "            'average_val_attacking', \n",
    "            aggregate(\n",
    "                array(*[col(pos) for pos in attacking_positions]),\n",
    "                lit(0.0),\n",
    "                lambda acc, x: acc + x,\n",
    "                lambda acc: acc / size(array(*[col(pos) for pos in attacking_positions]))\n",
    "            )\n",
    "        )\n",
    "        spark_df = spark_df.withColumn(\n",
    "            'average_val_midfield', \n",
    "            aggregate(\n",
    "                array(*[col(pos) for pos in midfield_positions]),\n",
    "                lit(0.0),\n",
    "                lambda acc, x: acc + x,\n",
    "                lambda acc: acc / size(array(*[col(pos) for pos in midfield_positions]))\n",
    "            )\n",
    "        )\n",
    "        spark_df = spark_df.withColumn(\n",
    "            'average_val_defensive', \n",
    "            aggregate(\n",
    "                array(*[col(pos) for pos in defensive_positions]),\n",
    "                lit(0.0),\n",
    "                lambda acc, x: acc + x,\n",
    "                lambda acc: acc / size(array(*[col(pos) for pos in defensive_positions]))\n",
    "            )\n",
    "        )\n",
    "        spark_df = spark_df.withColumn(\n",
    "            'average_val_gk', \n",
    "            aggregate(\n",
    "                array(*[col(pos) for pos in gk_positions]),\n",
    "                lit(0.0),\n",
    "                lambda acc, x: acc + x,\n",
    "                lambda acc: acc / size(array(*[col(pos) for pos in gk_positions]))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return spark_df\n",
    "\n",
    "def assign_traits(player_positions, *traits):\n",
    "        #Trait1 = pace/goalkeeping_diving\n",
    "        #Trait2 = shooting/goalkeeping_handling\n",
    "        #Trait3 = passing/goalkeeping_kicking\n",
    "        #Trait4 = dribbling/goalkeeping_positioning\n",
    "        #Trait5 = defending/goalkeeping_reflexes\n",
    "        #Trait6 = physic/goalkeeping_speed\n",
    "\n",
    "        traits = [float(t) if t is not None else None for t in traits]\n",
    "        if player_positions and 'GK' in player_positions:\n",
    "            return traits[6:12]\n",
    "        else:\n",
    "            return traits[:6]\n",
    "\n",
    "class MergeMainTraits(Transformer):\n",
    "    def __init__(self, main_traits_cols = None):\n",
    "        super().__init__()\n",
    "        self.main_traits_cols = main_traits_cols\n",
    "        \n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        assign_traits_udf = udf(assign_traits, ArrayType(FloatType()))\n",
    "        output_df = output_df.withColumn('traits', \n",
    "                                       assign_traits_udf(col('player_positions'), *[col(trait) for trait in self.main_traits_cols]))\n",
    "        for i in range(1, 7):\n",
    "            output_df = output_df.withColumn(f'trait{i}', col('traits').getItem(i-1))\n",
    "        \n",
    "        output_df = output_df.drop('traits')\n",
    "        return output_df\n",
    "    \n",
    "class DropOutliers(Transformer):\n",
    "    def __init__(self, continuous_cols = None):\n",
    "        super().__init__()\n",
    "        self.continuous_cols = continuous_cols\n",
    "\n",
    "    def column_add(self, a,b):\n",
    "        return  a.__add__(b)\n",
    "    \n",
    "    def find_outliers(self, df, continuous_cols):\n",
    "        # Identifying the numerical columns in a spark dataframe\n",
    "\n",
    "        # Using the `for` loop to create new columns by identifying the outliers for each feature\n",
    "        for column in continuous_cols:\n",
    "\n",
    "            less_Q1 = 'less_Q1_{}'.format(column)\n",
    "            more_Q3 = 'more_Q3_{}'.format(column)\n",
    "            Q1 = 'Q1_{}'.format(column)\n",
    "            Q3 = 'Q3_{}'.format(column)\n",
    "\n",
    "            # Q1 : First Quartile ., Q3 : Third Quartile\n",
    "            Q1 = df.approxQuantile(column,[0.25],relativeError=0)\n",
    "            Q3 = df.approxQuantile(column,[0.75],relativeError=0)\n",
    "            \n",
    "            # IQR : Inter Quantile Range\n",
    "            # We need to define the index [0], as Q1 & Q3 are a set of lists., to perform a mathematical operation\n",
    "            # Q1 & Q3 are defined seperately so as to have a clear indication on First Quantile & 3rd Quantile\n",
    "            IQR = Q3[0] - Q1[0]\n",
    "            \n",
    "            #selecting the data, with -1.5*IQR to + 1.5*IQR., where param = 1.5 default value\n",
    "            less_Q1 =  Q1[0] - 1.5*IQR\n",
    "            more_Q3 =  Q3[0] + 1.5*IQR\n",
    "            \n",
    "            isOutlierCol = 'is_outlier_{}'.format(column)\n",
    "            \n",
    "            df = df.withColumn(isOutlierCol,when((df[column] > more_Q3) | (df[column] < less_Q1), 1).otherwise(0))\n",
    "        \n",
    "\n",
    "        # Selecting the specific columns which we have added above, to check if there are any outliers\n",
    "        selected_columns = [column for column in df.columns if column.startswith(\"is_outlier\")]\n",
    "        # Adding all the outlier columns into a new colum \"total_outliers\", to see the total number of outliers\n",
    "        df = df.withColumn('total_outliers',reduce(self.column_add, ( df[col] for col in  selected_columns)))\n",
    "\n",
    "        # Dropping the extra columns created above, just to create nice dataframe., without extra columns\n",
    "        df = df.drop(*[column for column in df.columns if column.startswith(\"is_outlier\")])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        df_outliers = self.find_outliers(dataset, continuous_cols)\n",
    "        df_no_outliers = df_outliers.filter(df_outliers['total_outliers']<=6) #Drop all rows with more than 6 outliers\n",
    "        df_no_outliers = df_no_outliers.drop(\"total_outliers\")\n",
    "        return df_no_outliers    \n",
    "     \n",
    "class DropNullRows(Transformer):\n",
    "    def __init__(self, dropped_null_rows = None):\n",
    "        super().__init__()\n",
    "        self.null_cols_drop = dropped_null_rows\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset.na.drop(subset=self.null_cols_drop)\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in (continuous_cols + imputed_cols):\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "        return output_df\n",
    "    \n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing_pipeline():\n",
    "    #Drop all initial columns that are not needed\n",
    "    stage_dropper = ColumnDropper(columns_to_drop = columns_to_drop + null_cols_drop + corr_cols_drop)\n",
    "\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    #Engineer features\n",
    "    stage_maintraits = MergeMainTraits(main_traits_cols)\n",
    "    trait_cols = ['trait1', 'trait2', 'trait3', 'trait4', 'trait5', 'trait6']\n",
    "\n",
    "    stage_average_positions = AveragePositionCreator(position_cols)\n",
    "    avg_position_cols = ['average_val_attacking', 'average_val_midfield', 'average_val_defensive', 'average_val_gk']\n",
    "   \n",
    "    stage_average_skills = AverageSkillCreator(skill_cols)\n",
    "    avg_skill_cols = [\"corr_av_skills\"]\n",
    "\n",
    "    #Impute features\n",
    "    imputed_id_cols = [x+\"_imputed\" for x in imputed_cols]\n",
    "    stage_imputer = NullImputer()\n",
    "    stage_ml_imputer = MLImputer()\n",
    "    \n",
    "    #Stage where null rows are dropped based on certain features\n",
    "    stage_null_dropper = DropNullRows(dropped_null_rows)\n",
    "\n",
    "    #Stage where outlier rows are dropped for continuous features\n",
    "    stage_outlier_dropper = DropOutliers(continuous_cols)\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where ordinal columns are transformed to index columns using StringIndexer\n",
    "    ordinal_id_cols = [x+\"_index\" for x in ordinal_cols]\n",
    "    stage_ordinal_indexer = StringIndexer(inputCols = ordinal_cols, outputCols = ordinal_id_cols )\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols + nominal_onehot_cols + ordinal_id_cols + trait_cols + avg_position_cols + avg_skill_cols + imputed_id_cols #[imputed_id_cols[1]]\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "    \n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = feature_cols + main_traits_cols + \n",
    "                                         position_cols + skill_cols + imputed_cols + needed_cols + ordinal_cols + nominal_cols + nominal_id_cols +['vectorized_features'])\n",
    "    \n",
    "    # Connect the columns into a pipeline\n",
    "    '''\n",
    "    pipeline = Pipeline(stages=[stage_dropper, stage_typecaster, stage_maintraits, stage_average_positions, stage_average_skills,\n",
    "                                stage_imputer, stage_ml_imputer, stage_null_dropper, stage_outlier_dropper,\n",
    "                                stage_nominal_indexer, stage_nominal_onehot_encoder, stage_ordinal_indexer,\n",
    "                                stage_vector_assembler, stage_scaler, stage_column_dropper])\n",
    "    '''\n",
    "\n",
    "    pipeline = Pipeline(stages=[stage_dropper, stage_typecaster, stage_maintraits, stage_average_positions, stage_average_skills,\n",
    "                                stage_imputer, stage_ml_imputer, stage_null_dropper, stage_nominal_indexer, stage_nominal_onehot_encoder, stage_ordinal_indexer,\n",
    "                                stage_vector_assembler, stage_scaler, stage_column_dropper])\n",
    "    # pipeline = Pipeline(stages=[stage_ml_imputer])\n",
    "\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_from_spark(spark, db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1503:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|overall|            features|\n",
      "+-------+--------------------+\n",
      "|     69|[11.1535470006512...|\n",
      "|     69|[11.4722197720984...|\n",
      "|     64|[10.9942106149276...|\n",
      "|     69|[10.9942106149276...|\n",
      "|     86|[14.0216019436758...|\n",
      "|     86|[14.4996111008465...|\n",
      "|     66|[12.5875744721635...|\n",
      "|     85|[13.5435927865050...|\n",
      "|     63|[10.6755378434804...|\n",
      "|     69|[11.4722197720984...|\n",
      "|     64|[11.7908925435455...|\n",
      "|     69|[11.4722197720984...|\n",
      "|     58|[11.1535470006512...|\n",
      "|     64|[12.2689017007163...|\n",
      "|     65|[10.8348742292040...|\n",
      "|     80|[12.7469108578871...|\n",
      "|     80|[12.7469108578871...|\n",
      "|     68|[10.8348742292040...|\n",
      "|     78|[12.4282380864399...|\n",
      "|     63|[10.0381923005861...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = get_preprocessing_pipeline()\n",
    "preprocess_model = pipeline.fit(df)\n",
    "df_clean = preprocess_model.transform(df)\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.withColumn('overall', col('overall').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SparkML</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression as spark_LR, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80/20 split into Train and Test\n",
    "train_data, test_data = df_clean.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "def get_ML_pipeline(ML_model):\n",
    "    if ML_model == \"LR\":\n",
    "        ml = spark_LR(featuresCol='features', labelCol='overall')\n",
    "        \n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(ml.regParam, [0.01, 0.5, 2.0])\n",
    "                .addGrid(ml.maxIter, [1, 5, 10])\n",
    "                .build())\n",
    "        \n",
    "    elif ML_model == \"DT\":\n",
    "        ml = DecisionTreeRegressor(featuresCol='features', labelCol='overall')\n",
    "\n",
    "        paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(ml.minInfoGain, [0.0, 0.1, 0.2])\n",
    "                .addGrid(ml.minInstancesPerNode, [1, 2])\n",
    "                .build())\n",
    "    \n",
    "    evaluator = RegressionEvaluator(predictionCol='prediction', \n",
    "            labelCol='overall', metricName='r2')\n",
    "    \n",
    "    stage_ML = CrossValidator(estimator=ml, estimatorParamMaps=paramGrid, \n",
    "                           evaluator=evaluator, numFolds=5)\n",
    "    \n",
    "    pipeline = Pipeline(stages=[stage_ML])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code for training, hyperparametertuning, testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT train r2 score: 93.05%\n",
      "DT test r2 score: 92.87%\n",
      "Best DT parameters: {Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='featuresCol', doc='features column name.'): 'features', Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance', Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='labelCol', doc='label column name.'): 'overall', Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '', Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32, Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0, Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='DecisionTreeRegressor_75bf3b076d8c', name='seed', doc='random seed.'): -1407754390808368278}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2849:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR train r2 score: 93.59%\n",
      "LR test r2 score: 93.53%\n",
      "Best LR parameters: {Param(parent='LinearRegression_415e5ac84707', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LinearRegression_415e5ac84707', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LinearRegression_415e5ac84707', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35, Param(parent='LinearRegression_415e5ac84707', name='featuresCol', doc='features column name.'): 'features', Param(parent='LinearRegression_415e5ac84707', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LinearRegression_415e5ac84707', name='labelCol', doc='label column name.'): 'overall', Param(parent='LinearRegression_415e5ac84707', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError', Param(parent='LinearRegression_415e5ac84707', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LinearRegression_415e5ac84707', name='maxIter', doc='max number of iterations (>= 0).'): 1, Param(parent='LinearRegression_415e5ac84707', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LinearRegression_415e5ac84707', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LinearRegression_415e5ac84707', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto', Param(parent='LinearRegression_415e5ac84707', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LinearRegression_415e5ac84707', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ML_model = [\"DT\", \"LR\"]\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', \n",
    "            labelCol='overall', metricName='r2')\n",
    "\n",
    "for model in ML_model:\n",
    "    ML_pipeline = get_ML_pipeline(model)\n",
    "    ML_model_pipeline = ML_pipeline.fit(train_data) #training\n",
    "    model_df_train = ML_model_pipeline.transform(train_data) #train predictions\n",
    "    model_df_test = ML_model_pipeline.transform(test_data)  #test predictions\n",
    "\n",
    "    r2_train = evaluator.evaluate(model_df_train)\n",
    "    r2_test = evaluator.evaluate(model_df_test)\n",
    "    print(f\"{model} train r2 score: {r2_train}\")\n",
    "    print(f\"{model} test r2 score: {r2_test}\")\n",
    "\n",
    "    best_model = ML_model_pipeline.stages[-1].bestModel\n",
    "    best_params = best_model.extractParamMap()\n",
    "    print(f\"Best {model} parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Output after ~10mins</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT train r2 score: 93.05%\n",
      "DT test r2 score: 92.87%\n",
      "Best DT parameters: {Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='featuresCol', doc='features column name.'): 'features', Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance', Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='labelCol', doc='label column name.'): 'overall', Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '', Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32, Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5, Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0, Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='DecisionTreeRegressor_ab8a1d978e02', name='seed', doc='random seed.'): -1407754390808368278}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/14 12:53:49 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/11/14 12:53:49 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/11/14 12:53:49 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "[Stage 1418:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR train r2 score: 93.59%\n",
      "LR test r2 score: 93.53%\n",
      "Best LR parameters: {Param(parent='LinearRegression_331661238a01', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LinearRegression_331661238a01', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LinearRegression_331661238a01', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35, Param(parent='LinearRegression_331661238a01', name='featuresCol', doc='features column name.'): 'features', Param(parent='LinearRegression_331661238a01', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LinearRegression_331661238a01', name='labelCol', doc='label column name.'): 'overall', Param(parent='LinearRegression_331661238a01', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError', Param(parent='LinearRegression_331661238a01', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LinearRegression_331661238a01', name='maxIter', doc='max number of iterations (>= 0).'): 1, Param(parent='LinearRegression_331661238a01', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LinearRegression_331661238a01', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LinearRegression_331661238a01', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto', Param(parent='LinearRegression_331661238a01', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LinearRegression_331661238a01', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ML_model = [\"DT\", \"LR\"]\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction', \n",
    "            labelCol='overall', metricName='r2')\n",
    "\n",
    "for model in ML_model:\n",
    "    ML_pipeline = get_ML_pipeline(model)\n",
    "    ML_model_pipeline = ML_pipeline.fit(train_data) #training\n",
    "    model_df_train = ML_model_pipeline.transform(train_data) #train predictions\n",
    "    model_df_test = ML_model_pipeline.transform(test_data)  #test predictions\n",
    "\n",
    "    r2_train = evaluator.evaluate(model_df_train)\n",
    "    r2_test = evaluator.evaluate(model_df_test)\n",
    "    print(f\"{model} train r2 score: {r2_train}\")\n",
    "    print(f\"{model} test r2 score: {r2_test}\")\n",
    "\n",
    "    best_model = ML_model_pipeline.stages[-1].bestModel\n",
    "    best_params = best_model.extractParamMap()\n",
    "    print(f\"Best {model} parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pytorch code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "s = df_clean.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = s.drop('overall', axis=1)\n",
    "y = s['overall']\n",
    "# Split the data into train+validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Further split train+validation into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(np.array(X_train['features'].values.tolist(), np.float32))\n",
    "X_val = torch.from_numpy(np.array(X_val['features'].values.tolist(), np.float32))\n",
    "X_test = torch.from_numpy(np.array(X_test['features'].values.tolist(), np.float32))\n",
    "y_train = torch.from_numpy(np.array(y_train, np.float32))\n",
    "y_val = torch.from_numpy(np.array(y_val, np.float32))\n",
    "y_test = torch.from_numpy(np.array(y_test, np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).reshape(-1, 1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Multi-layer Perceptron (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_true = []\n",
    "        train_pred = []\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            train_true.extend(targets.cpu().numpy())\n",
    "            train_pred.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_true = []\n",
    "        val_pred = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                \n",
    "                val_true.extend(targets.cpu().numpy())\n",
    "                val_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_r2 = r2_score(train_true, train_pred)\n",
    "        val_r2 = r2_score(val_true, val_pred)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, Train R2: {train_r2:.4f}, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val R2: {val_r2:.4f}')\n",
    "    \n",
    "    return val_loss, val_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(model_class, param_grid):\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    best_val_r2 = -float('inf')\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        current_params = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {current_params}\")\n",
    "\n",
    "        if model_class == MLP:\n",
    "            model = model_class(input_size=X_train.shape[1], hidden_size=current_params['hidden_size'])\n",
    "        elif model_class == LinearRegression:\n",
    "            model = model_class(input_size=X_train.shape[1])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model class\")\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=current_params['lr'])\n",
    "        criterion = nn.MSELoss()\n",
    "        train_loader = DataLoader(train_dataset, batch_size=current_params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=current_params['batch_size'])\n",
    "\n",
    "        val_loss, val_r2 = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_r2 = val_r2\n",
    "            best_val_loss = val_loss\n",
    "            best_params = current_params\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "    return best_params, best_val_loss, best_model, best_val_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(model_class):\n",
    "    input_size = X_test_tensor.shape[1]\n",
    "    if model_class == \"LR\":\n",
    "        model = LinearRegression(input_size)\n",
    "        model.load_state_dict(torch.load('./best_linear_model.pth'))\n",
    "    elif model_class == \"MLP\":\n",
    "        model = MLP(input_size, mlp_best_params['hidden_size'])\n",
    "        model.load_state_dict(torch.load('./best_mlp_model.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "    y_pred_np = y_pred.cpu().numpy()\n",
    "    y_test_np = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    r2 = r2_score(y_test_np, y_pred_np)\n",
    "    mse = mean_squared_error(y_test_np, y_pred_np)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids\n",
    "mlp_param_grid = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'batch_size': [32, 64],\n",
    "    'hidden_size': [32, 64]\n",
    "}\n",
    "linear_param_grid = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'batch_size': [32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>To just start hyperprameter tuning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune and train models\n",
    "print(\"Tuning MLP model...\")\n",
    "mlp_best_params, mlp_best_loss, mlp_best_model, mlp_best_r2 = tune_hyperparameters(MLP, mlp_param_grid)\n",
    "print(f\"Best MLP parameters: {mlp_best_params}\")\n",
    "print(f\"Best MLP validation loss: {mlp_best_loss}\")\n",
    "print(f\"Best MLP validation r2: {mlp_best_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>These are the results you get after ~1.5hrs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning MLP model...\n",
      "Training with parameters: {'lr': 0.001, 'batch_size': 32, 'hidden_size': 32}\n",
      "Epoch [10/100], Train Loss: 1.1208, Train R2: 0.9776, Val Loss: 1.0210, Val R2: 0.9794\n",
      "Epoch [20/100], Train Loss: 0.9914, Train R2: 0.9802, Val Loss: 0.9165, Val R2: 0.9815\n",
      "Epoch [30/100], Train Loss: 0.9536, Train R2: 0.9810, Val Loss: 1.0133, Val R2: 0.9796\n",
      "Epoch [40/100], Train Loss: 0.9319, Train R2: 0.9814, Val Loss: 1.0564, Val R2: 0.9787\n",
      "Epoch [50/100], Train Loss: 0.9126, Train R2: 0.9818, Val Loss: 0.8816, Val R2: 0.9822\n",
      "Epoch [60/100], Train Loss: 0.8908, Train R2: 0.9822, Val Loss: 0.9425, Val R2: 0.9810\n",
      "Epoch [70/100], Train Loss: 0.8801, Train R2: 0.9824, Val Loss: 0.8150, Val R2: 0.9836\n",
      "Epoch [80/100], Train Loss: 0.8693, Train R2: 0.9827, Val Loss: 0.9018, Val R2: 0.9818\n",
      "Epoch [90/100], Train Loss: 0.8462, Train R2: 0.9831, Val Loss: 0.8127, Val R2: 0.9836\n",
      "Epoch [100/100], Train Loss: 0.8287, Train R2: 0.9835, Val Loss: 0.7972, Val R2: 0.9839\n",
      "Training with parameters: {'lr': 0.001, 'batch_size': 32, 'hidden_size': 64}\n",
      "Epoch [10/100], Train Loss: 1.0395, Train R2: 0.9793, Val Loss: 0.9582, Val R2: 0.9807\n",
      "Epoch [20/100], Train Loss: 0.9282, Train R2: 0.9815, Val Loss: 0.9063, Val R2: 0.9817\n",
      "Epoch [30/100], Train Loss: 0.8575, Train R2: 0.9829, Val Loss: 0.8740, Val R2: 0.9824\n",
      "Epoch [40/100], Train Loss: 0.7691, Train R2: 0.9847, Val Loss: 0.7294, Val R2: 0.9853\n",
      "Epoch [50/100], Train Loss: 0.7487, Train R2: 0.9851, Val Loss: 0.7509, Val R2: 0.9849\n",
      "Epoch [60/100], Train Loss: 0.7216, Train R2: 0.9856, Val Loss: 0.7324, Val R2: 0.9852\n",
      "Epoch [70/100], Train Loss: 0.6970, Train R2: 0.9861, Val Loss: 0.6590, Val R2: 0.9867\n",
      "Epoch [80/100], Train Loss: 0.6826, Train R2: 0.9864, Val Loss: 0.7141, Val R2: 0.9856\n",
      "Epoch [90/100], Train Loss: 0.6664, Train R2: 0.9867, Val Loss: 0.6415, Val R2: 0.9871\n",
      "Epoch [100/100], Train Loss: 0.6638, Train R2: 0.9868, Val Loss: 0.8688, Val R2: 0.9825\n",
      "Training with parameters: {'lr': 0.001, 'batch_size': 64, 'hidden_size': 32}\n",
      "Epoch [10/100], Train Loss: 1.8943, Train R2: 0.9622, Val Loss: 1.8454, Val R2: 0.9628\n",
      "Epoch [20/100], Train Loss: 1.4730, Train R2: 0.9706, Val Loss: 1.4174, Val R2: 0.9714\n",
      "Epoch [30/100], Train Loss: 1.3897, Train R2: 0.9723, Val Loss: 1.3211, Val R2: 0.9734\n",
      "Epoch [40/100], Train Loss: 1.3251, Train R2: 0.9736, Val Loss: 1.2743, Val R2: 0.9743\n",
      "Epoch [50/100], Train Loss: 1.3089, Train R2: 0.9739, Val Loss: 1.2820, Val R2: 0.9742\n",
      "Epoch [60/100], Train Loss: 1.2851, Train R2: 0.9744, Val Loss: 1.2426, Val R2: 0.9749\n",
      "Epoch [70/100], Train Loss: 1.2657, Train R2: 0.9748, Val Loss: 1.2460, Val R2: 0.9749\n",
      "Epoch [80/100], Train Loss: 1.2540, Train R2: 0.9750, Val Loss: 1.2613, Val R2: 0.9746\n",
      "Epoch [90/100], Train Loss: 1.2577, Train R2: 0.9749, Val Loss: 1.2840, Val R2: 0.9741\n",
      "Epoch [100/100], Train Loss: 1.2384, Train R2: 0.9753, Val Loss: 1.1974, Val R2: 0.9758\n",
      "Training with parameters: {'lr': 0.001, 'batch_size': 64, 'hidden_size': 64}\n",
      "Epoch [10/100], Train Loss: 1.6235, Train R2: 0.9676, Val Loss: 1.6584, Val R2: 0.9666\n",
      "Epoch [20/100], Train Loss: 1.3705, Train R2: 0.9727, Val Loss: 1.3222, Val R2: 0.9733\n",
      "Epoch [30/100], Train Loss: 1.2888, Train R2: 0.9743, Val Loss: 1.3747, Val R2: 0.9723\n",
      "Epoch [40/100], Train Loss: 1.2552, Train R2: 0.9750, Val Loss: 1.1816, Val R2: 0.9762\n",
      "Epoch [50/100], Train Loss: 1.2317, Train R2: 0.9754, Val Loss: 1.2154, Val R2: 0.9755\n",
      "Epoch [60/100], Train Loss: 1.2120, Train R2: 0.9758, Val Loss: 1.1769, Val R2: 0.9763\n",
      "Epoch [70/100], Train Loss: 1.1889, Train R2: 0.9763, Val Loss: 1.1682, Val R2: 0.9764\n",
      "Epoch [80/100], Train Loss: 1.1821, Train R2: 0.9764, Val Loss: 1.2399, Val R2: 0.9750\n",
      "Epoch [90/100], Train Loss: 1.1554, Train R2: 0.9770, Val Loss: 1.2407, Val R2: 0.9750\n",
      "Epoch [100/100], Train Loss: 1.1674, Train R2: 0.9767, Val Loss: 1.1230, Val R2: 0.9774\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 32, 'hidden_size': 32}\n",
      "Epoch [10/100], Train Loss: 1.2482, Train R2: 0.9751, Val Loss: 1.0193, Val R2: 0.9795\n",
      "Epoch [20/100], Train Loss: 1.0558, Train R2: 0.9789, Val Loss: 1.0253, Val R2: 0.9793\n",
      "Epoch [30/100], Train Loss: 1.0023, Train R2: 0.9800, Val Loss: 0.8606, Val R2: 0.9826\n",
      "Epoch [40/100], Train Loss: 0.9755, Train R2: 0.9805, Val Loss: 0.9232, Val R2: 0.9814\n",
      "Epoch [50/100], Train Loss: 0.9658, Train R2: 0.9807, Val Loss: 0.9370, Val R2: 0.9811\n",
      "Epoch [60/100], Train Loss: 0.9729, Train R2: 0.9806, Val Loss: 0.8750, Val R2: 0.9824\n",
      "Epoch [70/100], Train Loss: 0.9567, Train R2: 0.9809, Val Loss: 1.1393, Val R2: 0.9770\n",
      "Epoch [80/100], Train Loss: 0.9503, Train R2: 0.9810, Val Loss: 0.8844, Val R2: 0.9822\n",
      "Epoch [90/100], Train Loss: 0.9549, Train R2: 0.9810, Val Loss: 1.1118, Val R2: 0.9776\n",
      "Epoch [100/100], Train Loss: 0.9549, Train R2: 0.9810, Val Loss: 0.8383, Val R2: 0.9831\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 32, 'hidden_size': 64}\n",
      "Epoch [10/100], Train Loss: 1.1743, Train R2: 0.9766, Val Loss: 1.5922, Val R2: 0.9679\n",
      "Epoch [20/100], Train Loss: 1.0802, Train R2: 0.9785, Val Loss: 0.8595, Val R2: 0.9827\n",
      "Epoch [30/100], Train Loss: 1.0276, Train R2: 0.9795, Val Loss: 0.9836, Val R2: 0.9802\n",
      "Epoch [40/100], Train Loss: 0.9965, Train R2: 0.9801, Val Loss: 0.9082, Val R2: 0.9817\n",
      "Epoch [50/100], Train Loss: 0.9842, Train R2: 0.9804, Val Loss: 0.9762, Val R2: 0.9803\n",
      "Epoch [60/100], Train Loss: 0.9815, Train R2: 0.9804, Val Loss: 0.9039, Val R2: 0.9818\n",
      "Epoch [70/100], Train Loss: 0.9857, Train R2: 0.9803, Val Loss: 0.8758, Val R2: 0.9823\n",
      "Epoch [80/100], Train Loss: 0.9770, Train R2: 0.9805, Val Loss: 1.4528, Val R2: 0.9707\n",
      "Epoch [90/100], Train Loss: 0.8978, Train R2: 0.9821, Val Loss: 0.7503, Val R2: 0.9849\n",
      "Epoch [100/100], Train Loss: 0.8666, Train R2: 0.9827, Val Loss: 0.9379, Val R2: 0.9811\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 64, 'hidden_size': 32}\n",
      "Epoch [10/100], Train Loss: 1.8593, Train R2: 0.9629, Val Loss: 1.6818, Val R2: 0.9661\n",
      "Epoch [20/100], Train Loss: 1.0808, Train R2: 0.9784, Val Loss: 0.9244, Val R2: 0.9814\n",
      "Epoch [30/100], Train Loss: 1.0057, Train R2: 0.9799, Val Loss: 1.0554, Val R2: 0.9787\n",
      "Epoch [40/100], Train Loss: 0.9044, Train R2: 0.9820, Val Loss: 0.7922, Val R2: 0.9840\n",
      "Epoch [50/100], Train Loss: 0.8614, Train R2: 0.9828, Val Loss: 0.7976, Val R2: 0.9839\n",
      "Epoch [60/100], Train Loss: 0.8235, Train R2: 0.9836, Val Loss: 0.8538, Val R2: 0.9828\n",
      "Epoch [70/100], Train Loss: 0.8035, Train R2: 0.9840, Val Loss: 0.7632, Val R2: 0.9846\n",
      "Epoch [80/100], Train Loss: 0.7949, Train R2: 0.9841, Val Loss: 0.7156, Val R2: 0.9856\n",
      "Epoch [90/100], Train Loss: 0.7878, Train R2: 0.9843, Val Loss: 0.7396, Val R2: 0.9851\n",
      "Epoch [100/100], Train Loss: 0.7650, Train R2: 0.9847, Val Loss: 0.7383, Val R2: 0.9851\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 64, 'hidden_size': 64}\n",
      "Epoch [10/100], Train Loss: 1.0555, Train R2: 0.9789, Val Loss: 1.0581, Val R2: 0.9787\n",
      "Epoch [20/100], Train Loss: 0.8421, Train R2: 0.9832, Val Loss: 0.7592, Val R2: 0.9847\n",
      "Epoch [30/100], Train Loss: 0.7894, Train R2: 0.9843, Val Loss: 0.6862, Val R2: 0.9862\n",
      "Epoch [40/100], Train Loss: 0.7416, Train R2: 0.9852, Val Loss: 0.7230, Val R2: 0.9854\n",
      "Epoch [50/100], Train Loss: 0.7437, Train R2: 0.9852, Val Loss: 1.1571, Val R2: 0.9767\n",
      "Epoch [60/100], Train Loss: 0.7185, Train R2: 0.9857, Val Loss: 0.7042, Val R2: 0.9858\n",
      "Epoch [70/100], Train Loss: 0.7048, Train R2: 0.9859, Val Loss: 0.8354, Val R2: 0.9832\n",
      "Epoch [80/100], Train Loss: 0.6955, Train R2: 0.9861, Val Loss: 0.8337, Val R2: 0.9832\n",
      "Epoch [90/100], Train Loss: 0.6902, Train R2: 0.9862, Val Loss: 0.6218, Val R2: 0.9875\n",
      "Epoch [100/100], Train Loss: 0.6865, Train R2: 0.9863, Val Loss: 0.5878, Val R2: 0.9882\n",
      "Best MLP parameters: {'lr': 0.01, 'batch_size': 64, 'hidden_size': 64}\n",
      "Best MLP validation loss: 0.5877998555317904\n",
      "Best MLP validation r2: 0.9881517783577128\n"
     ]
    }
   ],
   "source": [
    "# Tune and train models\n",
    "print(\"Tuning MLP model...\")\n",
    "mlp_best_params, mlp_best_loss, mlp_best_model, mlp_best_r2 = tune_hyperparameters(MLP, mlp_param_grid)\n",
    "print(f\"Best MLP parameters: {mlp_best_params}\")\n",
    "print(f\"Best MLP validation loss: {mlp_best_loss}\")\n",
    "print(f\"Best MLP validation r2: {mlp_best_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameter tuning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTuning Linear Regression model...\")\n",
    "linear_best_params, linear_best_loss, linear_best_model, linear_best_r2 = tune_hyperparameters(LinearRegression, linear_param_grid)\n",
    "print(f\"Best Linear Regression parameters: {linear_best_params}\")\n",
    "print(f\"Best Linear Regression validation loss: {linear_best_loss}\")\n",
    "print(f\"Best Linear Regression validation R2: {linear_best_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Output after hyperparameter tuning ~40mins</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Linear Regression model...\n",
      "Training with parameters: {'lr': 0.001, 'batch_size': 32}\n",
      "Epoch [10/100], Train Loss: 3.5918, Train R2: 0.9284, Val Loss: 3.5292, Val R2: 0.9289\n",
      "Epoch [20/100], Train Loss: 3.3189, Train R2: 0.9338, Val Loss: 3.2670, Val R2: 0.9341\n",
      "Epoch [30/100], Train Loss: 3.2876, Train R2: 0.9344, Val Loss: 3.2376, Val R2: 0.9347\n",
      "Epoch [40/100], Train Loss: 3.2846, Train R2: 0.9345, Val Loss: 3.2352, Val R2: 0.9348\n",
      "Epoch [50/100], Train Loss: 3.2819, Train R2: 0.9345, Val Loss: 3.2496, Val R2: 0.9345\n",
      "Epoch [60/100], Train Loss: 3.2777, Train R2: 0.9346, Val Loss: 3.2134, Val R2: 0.9352\n",
      "Epoch [70/100], Train Loss: 3.2796, Train R2: 0.9346, Val Loss: 3.2179, Val R2: 0.9351\n",
      "Epoch [80/100], Train Loss: 3.2796, Train R2: 0.9346, Val Loss: 3.2810, Val R2: 0.9339\n",
      "Epoch [90/100], Train Loss: 3.2742, Train R2: 0.9347, Val Loss: 3.2217, Val R2: 0.9351\n",
      "Epoch [100/100], Train Loss: 3.2748, Train R2: 0.9347, Val Loss: 3.2043, Val R2: 0.9354\n",
      "Training with parameters: {'lr': 0.001, 'batch_size': 64}\n",
      "Epoch [10/100], Train Loss: 4.2905, Train R2: 0.9144, Val Loss: 4.1130, Val R2: 0.9171\n",
      "Epoch [20/100], Train Loss: 3.4382, Train R2: 0.9314, Val Loss: 3.3732, Val R2: 0.9320\n",
      "Epoch [30/100], Train Loss: 3.3147, Train R2: 0.9339, Val Loss: 3.2590, Val R2: 0.9343\n",
      "Epoch [40/100], Train Loss: 3.2909, Train R2: 0.9344, Val Loss: 3.2725, Val R2: 0.9340\n",
      "Epoch [50/100], Train Loss: 3.2810, Train R2: 0.9346, Val Loss: 3.2200, Val R2: 0.9351\n",
      "Epoch [60/100], Train Loss: 3.2779, Train R2: 0.9346, Val Loss: 3.2176, Val R2: 0.9351\n",
      "Epoch [70/100], Train Loss: 3.2705, Train R2: 0.9348, Val Loss: 3.2171, Val R2: 0.9352\n",
      "Epoch [80/100], Train Loss: 3.2688, Train R2: 0.9348, Val Loss: 3.2156, Val R2: 0.9352\n",
      "Epoch [90/100], Train Loss: 3.2718, Train R2: 0.9347, Val Loss: 3.2101, Val R2: 0.9353\n",
      "Epoch [100/100], Train Loss: 3.2700, Train R2: 0.9348, Val Loss: 3.2360, Val R2: 0.9348\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 32}\n",
      "Epoch [10/100], Train Loss: 3.4642, Train R2: 0.9309, Val Loss: 3.2343, Val R2: 0.9348\n",
      "Epoch [20/100], Train Loss: 3.4873, Train R2: 0.9304, Val Loss: 3.4029, Val R2: 0.9314\n",
      "Epoch [30/100], Train Loss: 3.5052, Train R2: 0.9301, Val Loss: 3.2399, Val R2: 0.9347\n",
      "Epoch [40/100], Train Loss: 3.4534, Train R2: 0.9311, Val Loss: 3.2181, Val R2: 0.9351\n",
      "Epoch [50/100], Train Loss: 3.4390, Train R2: 0.9314, Val Loss: 3.2143, Val R2: 0.9352\n",
      "Epoch [60/100], Train Loss: 3.4592, Train R2: 0.9310, Val Loss: 4.5478, Val R2: 0.9083\n",
      "Epoch [70/100], Train Loss: 3.4427, Train R2: 0.9313, Val Loss: 3.7018, Val R2: 0.9254\n",
      "Epoch [80/100], Train Loss: 3.4383, Train R2: 0.9314, Val Loss: 4.1265, Val R2: 0.9168\n",
      "Epoch [90/100], Train Loss: 3.4687, Train R2: 0.9308, Val Loss: 4.4385, Val R2: 0.9105\n",
      "Epoch [100/100], Train Loss: 3.4322, Train R2: 0.9315, Val Loss: 3.4019, Val R2: 0.9314\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 64}\n",
      "Epoch [10/100], Train Loss: 3.4232, Train R2: 0.9317, Val Loss: 3.2506, Val R2: 0.9345\n",
      "Epoch [20/100], Train Loss: 3.3851, Train R2: 0.9325, Val Loss: 3.2246, Val R2: 0.9350\n",
      "Epoch [30/100], Train Loss: 3.3900, Train R2: 0.9324, Val Loss: 3.4618, Val R2: 0.9302\n",
      "Epoch [40/100], Train Loss: 3.3992, Train R2: 0.9322, Val Loss: 3.2249, Val R2: 0.9350\n",
      "Epoch [50/100], Train Loss: 3.3924, Train R2: 0.9323, Val Loss: 3.3410, Val R2: 0.9327\n",
      "Epoch [60/100], Train Loss: 3.4337, Train R2: 0.9315, Val Loss: 3.2745, Val R2: 0.9340\n",
      "Epoch [70/100], Train Loss: 3.3699, Train R2: 0.9328, Val Loss: 3.2477, Val R2: 0.9345\n",
      "Epoch [80/100], Train Loss: 3.3937, Train R2: 0.9323, Val Loss: 3.2423, Val R2: 0.9347\n",
      "Epoch [90/100], Train Loss: 3.3877, Train R2: 0.9324, Val Loss: 3.2135, Val R2: 0.9352\n",
      "Epoch [100/100], Train Loss: 3.3710, Train R2: 0.9328, Val Loss: 3.2467, Val R2: 0.9346\n",
      "Best Linear Regression parameters: {'lr': 0.001, 'batch_size': 32}\n",
      "Best Linear Regression validation loss: 3.20432476140293\n",
      "Best Linear Regression validation R2: 0.9354013504086361\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTuning Linear Regression model...\")\n",
    "linear_best_params, linear_best_loss, linear_best_model, linear_best_r2 = tune_hyperparameters(LinearRegression, linear_param_grid)\n",
    "print(f\"Best Linear Regression parameters: {linear_best_params}\")\n",
    "print(f\"Best Linear Regression validation loss: {linear_best_loss}\")\n",
    "print(f\"Best Linear Regression validation R2: {linear_best_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save the best models (trained and tuned models are saved on github)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best models\n",
    "torch.save(mlp_best_model, 'best_mlp_model.pth')\n",
    "torch.save(linear_best_model, 'best_linear_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test results using best models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.9884\n",
      "Mean Squared Error: 0.5814\n",
      "Root Mean Squared Error: 0.7625\n"
     ]
    }
   ],
   "source": [
    "test_function(\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.9350\n",
      "Mean Squared Error: 3.2618\n",
      "Root Mean Squared Error: 1.8060\n"
     ]
    }
   ],
   "source": [
    "test_function(\"LR\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
